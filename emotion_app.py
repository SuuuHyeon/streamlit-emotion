# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mKKxL7s6WIJJ-7s3SethVWvP7O3dGMPH
"""

import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import cv2
import numpy as np
from deepface import DeepFace
import av # streamlit-webrtcë¥¼ ìœ„í•œ PyAV ë¼ì´ë¸ŒëŸ¬ë¦¬

st.set_page_config(page_title="ì‹¤ì‹œê°„ ê°ì • ë¶„ë¥˜ê¸°", page_icon="ğŸ˜Š")

st.title("ğŸ§  ì‹¤ì‹œê°„ ê°ì • ë¶„ë¥˜ê¸°")
st.write("ë…¸íŠ¸ë¶ ì¹´ë©”ë¼ë¥¼ í†µí•´ ë‹¹ì‹ ì˜ ê°ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
st.write("ì¹´ë©”ë¼ë¥¼ ì¼œê³  ì ì‹œ ê¸°ë‹¤ë¦¬ë©´ ì–¼êµ´ì„ ì¸ì‹í•˜ê³  ê°ì •ì„ í‘œì‹œí•©ë‹ˆë‹¤.")

# ê°ì • ë¶„ì„ ë° í™”ë©´ì— ê·¸ë¦¬ëŠ” ë¡œì§ì„ ë‹´ëŠ” í´ë˜ìŠ¤
class EmotionVideoTransformer(VideoTransformerBase):
    def __init__(self):
        # ì–¼êµ´ íƒì§€ë¥¼ ìœ„í•œ Haar Cascade ë¶„ë¥˜ê¸° ë¡œë“œ
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.emotion_cache = {} # ê°„ë‹¨í•œ ìºì‹œë¡œ ê°ì • ë¶„ì„ í˜¸ì¶œ ì¤„ì´ê¸°

    def transform(self, frame: av.VideoFrame) -> np.ndarray:
        img = frame.to_ndarray(format="bgr24")
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # ì–¼êµ´ íƒì§€
        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        for (x, y, w, h) in faces:
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            face_roi = img[y:y + h, x:x + w]

            try:
                # DeepFaceë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ë¶„ì„
                # enforce_detection=False: ì´ë¯¸ ì–¼êµ´ì„ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¬íƒì§€ ì•ˆí•¨
                analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)

                # DeepFace 2.0 ì´ìƒì—ì„œëŠ” analysisê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë¨
                if isinstance(analysis, list):
                    emotion = analysis[0]['dominant_emotion']
                else: # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
                    emotion = analysis['dominant_emotion']

                # ê°ì •ì„ ì˜ì–´ì—ì„œ í•œê¸€ë¡œ ë³€í™˜ (ì„ íƒì‚¬í•­)
                emotion_map = {
                    'angry': 'ë¶„ë…¸', 'disgust': 'í˜ì˜¤', 'fear': 'ë‘ë ¤ì›€',
                    'happy': 'í–‰ë³µ', 'sad': 'ìŠ¬í””', 'surprise': 'ë†€ëŒ', 'neutral': 'ì¤‘ë¦½'
                }
                emotion_korean = emotion_map.get(emotion, emotion)

                # ì–¼êµ´ ì£¼ìœ„ì— ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
                cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)

                # ê°ì • í…ìŠ¤íŠ¸ í‘œì‹œ
                font = cv2.FONT_HERSHEY_SIMPLEX
                cv2.putText(img, emotion_korean, (x, y - 10), font, 0.9, (255, 0, 0), 2)

            except Exception as e:
                # ì–¼êµ´ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ ê°ë„ê°€ ì•ˆ ë§ì•„ ë¶„ì„ì´ ì‹¤íŒ¨í•  ê²½ìš° ë¬´ì‹œ
                pass

        return img

# Streamlit-webrtc ìœ„ì ¯
webrtc_ctx = webrtc_streamer(
    key="emotion-classifier",
    video_transformer_factory=EmotionVideoTransformer,
    rtc_configuration={  # ICE ì„œë²„ ì„¤ì • (ë°°í¬ ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ)
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    },
    media_stream_constraints={"video": True, "audio": False},
    async_processing=True,
)

st.info("ì•±ì´ ì‹œì‘ë˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ì¹´ë©”ë¼ ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")